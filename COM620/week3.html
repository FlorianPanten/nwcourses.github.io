<html>
<head>
<title>Immersive Technologies - Topic 1</title>
<link rel='stylesheet' type='text/css' href='../css/dfti0910.css'>
</head>
<body>
<div class='titlebox'>
<h1>Immersive Technologies</h1>
<h2>Topic 3: Lighting and Textures</h2>
</div>
<hr />
<h2>Introduction</h2>
<p>Having covered the basics of setting up, and navigating through, a scene, this week we will look at how to develop more realistic three.js worlds making use of <em>lighting effects</em> and <em>textures</em>.</p>
<h2 id="materialsandlighting">Materials and lighting</h2>
<p>We're first going to look at how we can add simple <em>lighting</em> effects to our scene. Before we do that, we need to take a look at different types of <em>materials</em>.
<h3>Materials</h3>
<p>A word first on custom materials. So far, we have been using the <code>THREE.MeshBasicMaterial</code>. This very basic material does not take into account lighting effects. The material will appear the same, whether there are lights in our world or not. To account for the effect of lighting, we have to use more sophisticated materials which can reflect light. There are two commonly used materials:
<ul>
<li><code>THREE.MeshLambertMaterial</code> will reflect light shone on it.</li>
<li><code>THREE.MeshPhongMaterial</code> will also reflect light shone on it, but at a higher resolution, as the lighting effect is calculated per-pixel, rather than, as is the case for <code>THREE.MeshLambertMaterial</code> per-vertex. The result is a more 'shiny' effect, like a metallic surface. As a result, the lighting features <em>specular highlights</em>. Specular highlights are the small and intensely shiny spots of light you see on a metallic, or other shiny, surface at the point at which the light shines on its surface. A good example can be seen on cars on a sunny day, which feature small bright shiny spots at the point the sunlight shines on them.</li>
</ul>
<h3>Light types</h3>
<p>There are various light types available in three.js, each representing the different types of light we get in the real world. Four of the most commonly used are as follows:
<ul>
<li><p><code>THREE.AmbientLight</code>. Ambient light is the background lighting in our world. It does not radiate from a particular direction; there is no single light source. All objects, however they are oriented, will be lit by the ambient light. A good example of ambient light in the real world is the daylight on a cloudy day. You cannot see the sun but nonetheless the world is lit - perhaps rather dimly, but nonetheless still lit. There are no shadows, and everything is lit equally.</p></li>
<li><p><code>THREE.DirectionalLight</code>. A directional light is a light which is placed at a particular point within our world (for example, y=100) and shines in a particular direction, towards a <em>target</em> (which, by default, is the origin). Directional lights can be considered <em>infinitely far away</em>, so that they shine from a particular direction <em>no matter where in the world you are</em>. They are thus similar to the sun. So, for example, a directional light will shine fron the same direction whether you are at x=100, y=0, z=-100 or x=-500, y=10, z=-500, so boxes at each of these locations will be lit the same way (the top face of the box only will be lit if the directional light is shining straight down).</li>
<li><p><code>THREE.PointLight</code>. A point light is, like a directional light, placed at a particular point within our world. However, unlike a directional light, it shines in <em>all directions</em> and objects will be lit differently depending on what side of the point light they are placed. A point light is similar to a light bulb in the real world. So a box to the left of a point light will be illuminated on its right face, while a box to the right of a point light will be illuminated on its left face. Look at the diagram below showing a light bulb as a point light. The floor to the right of the bookcase is lit by the point light, but the floor to the left of the bookcase is not, because the bookcase blocks the light from the point light reaching it. Similarly only the right of the bookcase is lit by the point light, not the left.</li>
<li><p><code>THREE.SpotLight</code>. A spotlight behaves in much the same way as a real-world spotlight. A cone of light shines down from the spotlight, and the orientation of this cone depends on the orientation of the spotlight. Only objects within the cone will be lit. On the diagram, only a small section of the floor to the right of the bookcase, and part of the right-hand side of the bookcase, are lit. With a spotlight, the direction it shines, as well as the angle covered by the cone of light, can be defined.</li>
</ul>
<p><img src='../images/lighting.png' alt='Different light types' /></p>
<h4>Examples of the four light types</h4>
<p>Here is an incomplete code example (the code to setup the scene, camera and renderer is not present) of using all four light types. In general, when specifying light, you must specify its <em>colour</em> (again, as a hex code, just like for a material) and its <em>intensity</em> (a floating-point number between 0 and 1, with 1 indicating full intensity). For types other than ambient, you must also specify its <em>position</em>, and if you wish a directional light or spotlight to shine somewhere other than the origin, you must specify its <em>target</em>.
<pre>
// Create ambient light
// Specify colour and intensity
const light = new THREE.AmbientLight(0xffffff, 0.2);

// Add the light to the scene
scene.add(light);

// Create a directional light
const light2 = new THREE.DirectionalLight(0xffffff, 0.6);
light2.position.set(0, 100, 0); // x, y and z coordinates
scene.add(light2);

// Create a point light
const light3 = new THREE.PointLight(0xffffc0, 0.8);
light3.position.set(0, 10, 10);
scene.add(light3);

// Create a spotlight
const light4 = new THREE.SpotLight(0xffffff, 1.0);
light4.position.set(10, 10, -10);

// Change the spotlight's target to be the point x=0, y=0, z=-5
light4.target.position.set(0, 0, -5);

scene.add(light4);

// If the target changes, it must be added to the scene
// This also applies to directional lights
// See https://threejs.org/docs/index.html?q=SpotLight#api/en/lights/SpotLight.target
scene.add(light4.target);

// Create some shapes using Lambert and Phong materials

const redLambert = new THREE.MeshLambertMaterial({color: 0xff0000});
const greenPhong = new THREE.MeshPhongMaterial({
    color: 0x00ff00, 
    shininess: 60 // default is 30
});

const box = new THREE.BoxGeometry();
const sphere = new THREE.SphereGeometry();

const mesh1 = new THREE.Mesh(box, redLambert);
const mesh2 = new THREE.Mesh(sphere, greenPhong);

mesh1.position.set(0, 0, -10);
mesh2.position.set(0, 0, -5);

scene.add(mesh1);
scene.add(mesh2);
</pre>
</p>
<h4>Removing a light</h4>
<p>You can later remove a light (indeed, any <em>Object3D</em>, wbich also includes meshes; remember that an <em>Object3D</em> represents any object with a position which can be added to the scene) using the scene's <code>remove()</code> method, e.g.:
<pre>scene.remove(light4);</pre>
</code></pre>
<p>Just like the camera and meshes, lights are themselves <code>Object3D</code>s. This is why we can give them properties such as a position.</p>
<h3>Shadows</h3>
<p>To make our world even more realistic, we can add <em>shadow</em> effects, whereby an object will <em>cast a shadow</em> on a second object (often the ground) for areas on the second object in which the object blocks a light source. Shadows require relatively expensive computation, so if you don't need them, it's best to leave them out; in fact, by default, they are left out. However they can add nice realism to virtual-reality applications and games.</p>
<p>To create shadows, you need to do the following:
    <ul>
    <li>Turn on the renderer's <em>shadow map</em>:
    <pre>renderer.shadowMap.enabled = true;</pre>
    </li>
    <li>If an object can <em>cast a shadow</em>, i.e. that object is capable of blocking a light source, set its <code>castShadow</code> property to <em>true</em>:
    <pre>mesh1.castShadow = true;</pre>
    </li>
    <li>If an object can <em>receive a shadow</em>, i.e. shadows should be drawn on it if the light is blocked by another object, set <code>receiveShadow</code> to true:
    <pre>groundMesh.receiveShadow = true;</pre>
    </li>
    <li>Any <em>lights</em> capable of casting shadows must also have <em>castShadow</em> set to <em>true</em>:
    <pre>pointLight.castShadow = true;</pre>
    </li>
    </ul>
</p>
<h3>Planes</h3>
<p>We are talking about casting shadows on the ground, so this is a good point to introduce the <em>plane</em> geometry. The plane geometry represents a plane, or flat surface, and so is commonly used to represent the ground. When creating a plane geometry, we specify its width and height in world units:
<pre>const planeGeom = new THREE.PlaneGeometry(100, 100);</pre>
This will create a plane geometry of width 100, height 100, centred about the origin by default. However an important point is that by default, the <em>plane geometry is created in the <strong>xy-plane</strong></em>, i.e. all points on the plane will have constant <code>z</code>. Also by default, the plane faces positive <code>z</code>. This will have the effect of creating a plane that appears to stand up, vertically, facing positive <code>z</code>. (Planes are single-sided by default, though we can adjust this).</p>
<p>What we really need is a <em>ground</em> plane, i.e. a plane geometry in the <strong>xz-plane</strong>, with all <code>y</code> coordinates on the plane constant, and facing up, towards positive <code>y</code>. To do this, once we've created a mesh using the plane geometry and an appropriate material,  we need to rotate the plane by 90 degrees (0.5 * PI radians) clockwise about the <code>x</code> axis.  The diagram below clarifies the operations of rotating about the <code>x</code>, <code>y</code> and <code>z</code> axes: <br />
<img src="../images/axisrotation.png" alt="Rotation about the x, y and z axes" />
<br />
To do this, we set the mesh's <code>rotation</code> to -0.5*PI radians:
<pre>planeMesh.rotation.x = -Math.PI * 0.5;</pre>
</p>
<p>The effect on the plane is shown in the diagram below. Note how the plane is originally the xy-plane and we rotate it clockwise by 90 degrees so that it becomes the xz-plane:<br />
<img src="../images/default_plane_rotation.png" alt="Rotation of the default plane geometry" />
</p>
<p>There are various other things we can do with planes, for example we can define a plane mesh to consist of multiple vertices (points) a specific distance apart, such as a plane with width 100, height 100 made up of many points one unit apart. This is useful for generating <em>terrain</em>, by giving each vertex a different <code>y</code> coordinate representing its height above sea level in metres. We will return to this later.</p>
<h2>Textures</h2>
<p>We will now move onto the second main topic for this week: <em>textures</em>.</p>
<p>In a VR application or 3D game, most of the features of the 3D world don't have simple colours. Instead they have realistic-looking have <em>textures</em> such as brick walls, marble, metallic surfaces and so on. A texture is loaded from an image (such as a PNG image) and then associated with a material to be used as the surface for a mesh. The material can be any type of material (e.g. <code>MeshBasicMaterial</code>, <code>MeshLambertMaterial</code> and <code>MeshPhongMaterial</code>); the light reflectance will be defined by the material type rather than what is inside the PNG image.</p>
<h3>Loading a texture</h3>
<p>To load a texture, <code>THREE.TextureLoader</code> is used. For example:
<pre>
// Create a texture loader
const loader = new THREE.TextureLoader();

// Load an image to use as the texture
const texture = loader.load('metal.png');

// Create a material using the texture
const material = new THREE.MeshLambertMaterial({map: texture});

// Create a mesh
const mesh = new THREE.Mesh(boxGeom, material);

// Add it to the scene
scene.add(mesh);
</pre>
Note how the texture returned from <code>loader.load()</code> is used when creating a material, by specifying it as the material's <code>map</code> (texture map) property.</p>
<p>One issue with the above is that it might take time to load the texture, particularly if the network connection is slow. However <strong>the <code>load()</code> method returns immediately, even if the texture is not loaded.</strong> The result might be that the mesh is created before the texture has loaded, which will mean it will appear black initially, while waiting for the texture to load. We can resolve this by using the <code>TextureLoader</code> asynchronously, and specifying a <em>callback function</em> to run once the texture has loaded. For example:
<pre>
// Create a texture loader
const loader = new THREE.TextureLoader();

// Load an image to use as the texture
loader.load('metal.png', texture =&gt; {

    // Create a material using the texture
    const material = new THREE.MeshLambertMaterial({map: texture});

    // Create a mesh
    const mesh = new THREE.Mesh(boxGeom, material);

    // Add it to the scene
    scene.add(mesh);
});
</pre>
Note how in this version we supply a <em>callback function</em> to the <code>load()</code> method, which runs when the texture is fully loaded. This callback function takes the texture object as a parameter.</p>
<h3>Texture wrapping</h3>
<p><em>Texture wrapping</em> refers to the way in which the texture is applied to the mesh. By default, the texture is stretched over the whole mesh that it is applied to (this is called <em>clamping</em>). For a small mesh, that might be what we want (e.g. stretching an image containing the Coke logo over a cylinder to represent a Coke can) but for a larger mesh, such as a plane representing the ground, we probably don't want this. Let's say we have a texture image of 128x128 representing a floor tile. If this is stretched over the whole image it will appear blurred. We want instead to <em>tile</em> the texture image so that it repeats. We can do this by setting up <em>repeat wrapping</em>: 
<pre>
// Create a texture loader
const loader = new THREE.TextureLoader();

// Load an image to use as the texture
loader.load('floor.png', texture =&gt; {

    // The S axis of the texture is its horizontal axis (like x)
    texture.wrapS = THREE.RepeatWrapping;

    // The T axis of the texture is its vertical axis (like y)
    texture.wrapT = THREE.RepeatWrapping;

    // Make the texture repeat 64 times horizontally, 64 times vertically
    texture.repeat.set(64, 64);

    // Create a material using the texture
    const material = new THREE.MeshLambertMaterial({map: texture});

    // Create a mesh
    const mesh = new THREE.Mesh(planeGeom, material);

    // Add it to the scene
    scene.add(mesh);
});
</pre>
Note how we specify the <code>wrapS</code> and <code>wrapT</code> properties of the texture to specify how the texture is applied to the mesh in the horizontal (s) and vertical (t) directions. <code>THREE.RepeatWrapping</code> means that the texture will repeat in that direction. To specify the number of times the texture will repeat across the whole mesh, we set the properties of <code>texture.repeat</code>, the first parameter being horizontal and the second, vertical. Here, the texture will repeat 64 times both horizontally and vertically across the whole mesh.</p>
<h3>Texture magnification and minification</h3>
<p>Our texture occupies a certain number of pixels, for example 128x128. However the number of pixels that a mesh occupies while being displayed on screen is liely to be different to that - if the mesh is close to the camera it will likely occupy more pixels than the texture, but if it is far away it will likely occupy less. So the texture will have to be <em>magnified</em> or <em>minified</em> to fit on the mesh. There is not an exact correspondence between pixels of the texture image (called <em>texels</em>) and screen pixels.</p>
<p>In these cases, <em>magnification and minification filters</em> need to be applied. The diagram below shows a texture being minified (when applied to a distant mesh) and magnified (when applied to a nearby mesh).
<br />
<img src="../images/texture_mag_min.png" alt="Texture magnification and minification "/>
</p>
<h4>Magnification</h4>
<p>Magnification occurs when the texture occupies less pixels than the current view of the mesh it is to be applied to. For example, a 128x128 texture needs to be stretched across a face of a mesh occupying 256x256 screen pixels in the current view of the world. Two filter settings can be applied: 
<ul>
<li>One setting is <code>THREE.NearestFilter</code>. The <code>THREE.NearestFilter</code> will pick the nearest texture pixel (texel) to display on the mesh surface. For example, if the texture is 128x128 and is being displayed on a mesh which is occupying 256x256 screen pixels in the current view of the world, then the texel equal to the screen pixel divided by 2 (because 256/128 is 2) will be picked. So, mesh pixel x=200, y=200 will use texel x=100, y=100 as 200/2 is 100) but so will x=201, y=200; x=200, y=201; and x=201, y=201 (assuming rounding down during division). The end result of this will be that texel x=100, y=100 will be stretched across 2 screen pixels horizontally and 2 vertically, so 4 in total: x=200, y=200; x=201, y=200; x=200, y=201; x=201. y=201. This will lead to a very pixelated and blocky looking texture, as the texture is stretched across the mesh without any blending of the pixel colours occurring. So it is not ideal, and therefore not the default. It is computationally relatively fast, however, so may be chosen in low resource environments.</li>
<li>The default setting is to use <code>THREE.LinearFilter</code>. Rather than always picking the closest texel to a pixel, it merges the colours from the all the closest texels to that pixel to produce the final colour, weighting each texel depending on how close it is to the pixel. So for example if we divide screen pixel x=201, y=201 by two, we get x=100.5, y=100.5. So this screen pixel is in between four texels: x=100,y=100; x=101,y=100; x=100,y=101; and x=101, y=101. So the colours of all 4 texels are blended to give the colour for this pixel.</li>
</ul>
</p>
<p>The image below shows the difference. Note how with <code>NearestFilter</code> the nearest texel to a given pixel is always picked, while with <code>LinearFilter</code> blending of the colours of the nearby texels occurs, leading to a smoother and less pixelated effect.
<br />
<img src="../images/texturefilter.png" alt="Texture filters" />
</p>
<h4>Minification</h4>
<p>Minification is the opposite to magnification, and occurs when the current view of the mesh occupies <em>less</em> pixels than the texture, for example the texture is 128x128 texels but the face of the mesh that the texture is to be applied to occupies 64x64 screen pixels. So the texture has to be <em>minified</em> (shrunk) before being applied to the mesh. The above two filters are also used for minification, but alternatively, efficiency and rendering improvements can be made with <em>mipmaps</em>.
<p>A <em>mipmap</em> is a series of images of different dimensions in powers of 2, scaled down from the original texture. So for example if the texture is 128x128, there would be images of size 128x128, 64x64, 32x32, 16x16, 8x8, 4x4, 2x2 and 1x1. These are automatically created when a texture is loaded. For a mipmap to be created, the original texture must have power-of-2 dimensions.</p>
<p>The diagram below shows a series of mipmap images: note the smallest are too small to be shown visibly so the shown images range from 128x128 (the original image) to 8x8.
<br />
<img src="../images/mipmaps.png" alt="Mipmap" />
</p>
<p>
The mipmap image(s) most closely matching the current pixel dimensions of the mesh will be selected when applying the texture to the mesh. For example if the mesh, in the current view of the world, occupies 64x64 pixels, then the 64x64 pixel mipmap image will be chosen to be applied to the mesh. The advantage of mipmaps is that they are pre-computed when the texture is loaded, so that the texture does not need to be resized every time the visible size of the mesh changes, saving computational effort. Mipmaps also reduce display artefacts when the mesh is distant.</p>
<p>There are four different mipmap-based minification modes, described in the <a href='https://threejs.org/docs/index.html?q=textu#api/en/constants/Textures'>documentation</a>. Two of the modes blend adjacent mipmap images when the visible size of the mesh is not an exact power of two, for instance if the visible size is 96x96, texels fron the 128x128 and 64x64 mipmap images will be blended. The mode which gives the highest-quality display is <code>THREE.LinearMipmapLinearFilter</code> as that performs both texel and mipmap blending.</p>
</body>
</html>
